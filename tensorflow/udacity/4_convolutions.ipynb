{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 16]\n"
     ]
    }
   ],
   "source": [
    "t = test_labels[:10]\n",
    "a = np.arange(10)\n",
    "depth = 16\n",
    "num_hidden = 16\n",
    "#print(a)\n",
    "#print(t)\n",
    "#print(t[:, None])\n",
    "#print(train_dataset.shape)\n",
    "#print(train_dataset[0][0])\n",
    "t2 = train_dataset[:2]\n",
    "#print(t2.shape)\n",
    "t3 = t2.reshape((-1, 28, 28, 1))\n",
    "#print(t3.shape)\n",
    "image_size = 28\n",
    "print([image_size // 4 * image_size // 4 * depth, num_hidden])\n",
    "#b = (a == t[:, None]).astype(np.float32)\n",
    "#print(b)\n",
    "#print(train_dataset.shape)\n",
    "#c = np.zeros((10, 2))\n",
    "#print(c)\n",
    "#d = c.reshape((-1, 10, 2))\n",
    "#print(d.shape)\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  \n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.813728\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 12.2%\n",
      "Minibatch loss at step 50: 1.808777\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 50.9%\n",
      "Minibatch loss at step 100: 1.122523\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 150: 0.369430\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 200: 0.805793\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 250: 1.258946\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 300: 0.407371\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 350: 0.421301\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 400: 0.351300\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 450: 0.763328\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 500: 0.721722\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 550: 0.633133\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 600: 0.338527\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 650: 0.741943\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 700: 0.729605\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 750: 0.044565\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 800: 0.633134\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 850: 1.035199\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 900: 0.648110\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 950: 0.559094\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1000: 0.374665\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')    \n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    max_pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(max_pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')    \n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    max_pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = max_pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(max_pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden3, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.380093\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 50: 0.829961\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 100: 0.656380\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 150: 0.340694\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 200: 0.833252\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 250: 0.988732\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 300: 0.281218\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 350: 0.389162\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 400: 0.196215\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 450: 0.974135\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 500: 0.671064\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 550: 0.883579\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 600: 0.332615\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 650: 0.920615\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 700: 0.892168\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 750: 0.032775\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 800: 0.682308\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 850: 0.561251\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 900: 0.571963\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 950: 0.446184\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.182620\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 28, 28, 1) (200000,)\n",
      "Validation set (10000, 28, 28, 1) (10000,)\n",
      "Test set (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Using TF layers: tensorflow high-level API\n",
    "\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat_dataset(dataset):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "train_dataset = reformat_dataset(train_dataset)\n",
    "valid_dataset = reformat_dataset(valid_dataset)\n",
    "test_dataset = reformat_dataset(test_dataset)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "batch_size = 128\n",
    "patch_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def cnn_model(features, labels, mode):    \n",
    "    \"\"\"\n",
    "        CNN Architecture:\n",
    "            * Convolutional Layer #1: Applies 32 5x5 filters \n",
    "              (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "            * Pooling Layer #1: Performs max pooling with a 2x2 filter\n",
    "              and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "            * Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "            * Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "            * Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 \n",
    "              (probability of 0.4 that any given element will be dropped during training)\n",
    "            * Dense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0–9).\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features, [-1, image_size, image_size, 1])\n",
    "    # input shape: [batch_size, 28, 28, 1]\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(inputs=features, \n",
    "                             filters=32, \n",
    "                             kernel_size=[patch_size, patch_size],\n",
    "                             padding=\"same\",\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    # conv1 shape: [batch_size, 28, 28, 32]\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    # pool1 shape: [batch_size, 14, 14, 1]\n",
    "    \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                             filters=64,\n",
    "                             kernel_size=[patch_size, patch_size],\n",
    "                             padding=\"same\",\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    # conv2 shape: [batch_size, 14, 14, 64]\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    # pool2 shape: [batch_size, 7, 7, 1]\n",
    "    \n",
    "    # Dense layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n",
    "    \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:        \n",
    "        onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_labels)        \n",
    "\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),                                       \n",
    "            learning_rate=0.05,\n",
    "            optimizer='SGD')\n",
    "    \n",
    "    # Generate Predictions\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x17abf4450>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 3202 into /tmp/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0483, step = 3202\n",
      "INFO:tensorflow:global_step/sec: 12.6766\n",
      "INFO:tensorflow:loss = 1.36379, step = 3302\n",
      "INFO:tensorflow:Saving checkpoints for 3401 into /tmp/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.342576.\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-07-22:37:21\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-07-22:37:32\n",
      "INFO:tensorflow:Saving dict for global step 3401: accuracy = 0.8638, global_step = 3401, loss = 0.561103\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'loss': 0.56110269, 'global_step': 3401, 'accuracy': 0.86379999}\n"
     ]
    }
   ],
   "source": [
    "classifier = SKCompat(learn.Estimator(model_fn=cnn_model, model_dir=\"/tmp/convnet_model\"))\n",
    "\n",
    "# Logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "classifier.fit(\n",
    "    x=train_dataset,\n",
    "    y=train_labels,\n",
    "    batch_size=16,\n",
    "    steps=200,\n",
    "    monitors=None)\n",
    "\n",
    "# Configure the accuracy metric for evaluation\n",
    "metrics = {\n",
    "    \"accuracy\":\n",
    "        learn.MetricSpec(\n",
    "            metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\n",
    "}\n",
    "\n",
    "# Evaluate the model and print results\n",
    "eval_results = classifier.score(\n",
    "    x=test_dataset, y=test_labels, metrics=metrics)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "LeNet implementation using Tensorflow low level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "num_channels = 1\n",
    "image_size = 28\n",
    "dropout = 0.5\n",
    "graph2 = tf.Graph()\n",
    "\n",
    "with graph2.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=[batch_size, image_size, image_size, num_channels])\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, num_labels])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, 20], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([20]))\n",
    "\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, 20, 50], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[50]))\n",
    "\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [7 * 7 * 50, 500], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[500]))\n",
    "\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [500, 10], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "                                 \n",
    "    def model(data, training=False):\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        max_pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        conv2 = tf.nn.conv2d(max_pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "        max_pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        shape = max_pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(max_pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        if training:\n",
    "            keep_prob = tf.constant(dropout)\n",
    "        else:\n",
    "            keep_prob = tf.constant(1.0)\n",
    "            \n",
    "        hidden3_dropout = tf.nn.dropout(hidden3, keep_prob)\n",
    "        return tf.matmul(hidden3_dropout, layer4_weights) + layer4_biases\n",
    "\n",
    "    logits = model(tf_train_dataset, training=True)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    decay_steps = train_labels.shape[0]\n",
    "    decay_rate = 0.95\n",
    "    base_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        base_learning_rate, \n",
    "        global_step * batch_size, \n",
    "        decay_steps, \n",
    "        decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 13.756258\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 500: 0.410290\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1000: 0.562436\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1500: 0.304137\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 0.292231\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.331524\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.397565\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in xrange(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, keep_prob: dropout}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' %accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
